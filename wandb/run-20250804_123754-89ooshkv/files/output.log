LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(

  | Name  | Type | Params | Mode
---------------------------------------
0 | model | E2E  | 238 M  | train
---------------------------------------
238 M     Trainable params
0         Non-trainable params
238 M     Total params
955.192   Total estimated model params size (MB)
561       Modules in train mode
0         Modules in eval mode
Epoch 226:  44%|████████████████████████████████████████████████████                                                                  | 26/59 [03:27<04:22,  0.13it/s, v_num=shkv]
                                                                                                                                                                                  
/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
